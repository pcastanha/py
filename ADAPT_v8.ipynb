{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:\\Pedro\\workspace\\Acc-challenge\\data_v3_2.csv')\n",
    "#test = pd.read_csv('C:\\Pedro\\workspace\\Acc-challenge\\\\test_data_v3.csv')\n",
    "#train = train.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#concated_names = train['PlayerName'].append(test['PlayerName']).unique()\n",
    "#pname_mapping = dict(zip(sorted(concated_names), range(0, len(sorted(concated_names)) + 1)))\n",
    "pname_mapping = dict(zip(sorted(train['PlayerName'].unique()), range(0, len(sorted(train['PlayerName'].unique())) + 1)))\n",
    "aname_mapping = dict(zip(sorted(train['ActionName'].unique()), range(0, len(sorted(train['ActionName'].unique())) + 1)))\n",
    "atype_mapping = dict(zip(sorted(train['ActionTypeDesc'].unique()), range(0, len(sorted(train['ActionTypeDesc'].unique())) + 1)))\n",
    "spos_mapping = dict(zip(sorted(train['StartingPositionDesc'].unique()), range(0, len(sorted(train['StartingPositionDesc'].unique())) + 1)))\n",
    "epos_mapping = dict(zip(sorted(train['EndPositionDesc'].unique()), range(0, len(sorted(train['EndPositionDesc'].unique())) + 1)))\n",
    "stad_mapping = dict(zip(sorted(train['StadiumName'].unique()), range(0, len(sorted(train['StadiumName'].unique())) + 1)))\n",
    "team_mapping = dict(zip(sorted(train['Team'].unique()), range(0, len(sorted(train['Team'].unique())) + 1)))\n",
    "zone_mapping = dict(zip(sorted(train['ZoneDescription'].unique()), range(0, len(sorted(train['ZoneDescription'].unique())) + 1)))\n",
    "summary_mapping = dict(zip(sorted(train['Summary'].unique()), range(0, len(sorted(train['Summary'].unique())) + 1)))\n",
    "\n",
    "train['HomeTeam'].apply(lambda x: team_mapping.update(zip([x],[len(team_mapping.keys())])) \n",
    "                         if team_mapping.has_key(x) == False else None)\n",
    "\n",
    "train['AwayTeam'].apply(lambda x: team_mapping.update(zip([x],[len(team_mapping.keys())])) \n",
    "                         if team_mapping.has_key(x) == False else None)\n",
    "\n",
    "train['PlayerName_Val'] = train['PlayerName'].map(pname_mapping).astype(int)\n",
    "train['ActionName_Val'] = train['ActionName'].map(aname_mapping).astype(int)\n",
    "train['ActionType_Val'] = train['ActionTypeDesc'].map(atype_mapping).astype(int)\n",
    "train['StartingPositionDesc_Val'] = train['StartingPositionDesc'].map(spos_mapping).astype(int)\n",
    "train['EndPositionDesc_Val'] = train['EndPositionDesc'].map(epos_mapping).astype(int)\n",
    "train['StadiumName_Val'] = train['StadiumName'].map(stad_mapping).astype(int)\n",
    "train['Team_Val'] = train['Team'].map(team_mapping).astype(int)\n",
    "train['HomeTeam_Val'] = train['HomeTeam'].map(team_mapping).astype(int)\n",
    "train['AwayTeam_Val'] = train['AwayTeam'].map(team_mapping).astype(int)\n",
    "train['ZoneDescription_Val'] = train['ZoneDescription'].map(zone_mapping).astype(int)\n",
    "train['Summary_Val'] = train['Summary'].map(summary_mapping).astype(int)\n",
    "\n",
    "np.random.seed(0)\n",
    "train = train.reindex(np.random.permutation(train.index)) #Shuffling\n",
    "\n",
    "X_target = train['ActionName_Val'].values\n",
    "#X_train = train.drop(['ActionName_Val','ActionName','PlayerName','StartingPositionDesc',\n",
    "#                      'EndPositionDesc','Summary','RecordID','ActionTypeDesc','StadiumName',\n",
    "#                        'Team','HomeTeam','AwayTeam','ZoneDescription'], axis=1).values\n",
    "\n",
    "\n",
    "X_train = train.drop(['ActionName_Val','ActionName','PlayerName','StartingPositionDesc',\n",
    "                      'EndPositionDesc','Summary','RecordID','ActionTypeDesc','StadiumName',\n",
    "                        'Team','HomeTeam','AwayTeam','ZoneDescription','Visibility','Pressure',\n",
    "                         'Humidity','DewPoint','CloudCover','Temperature'], axis=1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting data into train, cv and test sets using 60%, 20%, 20% proportions\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, X_target, test_size=0.4, random_state=42)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X_cv, y_cv, test_size=0.5, random_state=42)\n",
    "\n",
    "# Using only Train and CV: 80-20\n",
    "#X_train, X_cv, y_train, y_cv = train_test_split(X_train, X_target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_normalized = scaler.transform(X_train)\n",
    "X_cv_normalized = scaler.transform(X_cv)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "\n",
    "rate_param = np.logspace(-5,1,10)\n",
    "\n",
    "#tuned_parameters = [{'max_features': [None,'auto'],'min_samples_leaf': [1,2,5,10,15],\n",
    "#                     'max_depth': [None,1,5,10,20,25]}] # DecisionTree\n",
    "#tuned_parameters = [{'max_features': [4,6,8,10,12,14,16,20,22,24], 'n_estimators': [10, 50, 100, 200, 400]}] #RandomForest\n",
    "#tuned_parameters = [{'n_estimators': [10, 50, 100, 200, 400,600,700,800], 'min_samples_leaf': [1,2,4,6,8,16,32],\n",
    "#                       'class_weight':['balanced',None]}] # ExtraTree\n",
    "#tuned_parameters = [{'n_estimators': [200, 400,600,700,800], 'min_samples_leaf': [16,32,64],\n",
    "#                       'class_weight':['balanced']}] # ExtraTree\n",
    "#tuned_parameters = [{'learning_rate': [1.4,1.6,1.2,1.0,0.8,0.6,0.4],\n",
    "#                     'n_estimators': [10, 25, 50, 75, 100, 200, 400, 600]}] #AdaBoost\n",
    "\n",
    "tuned_parameters = [{'learning_rate': rate_param,'n_estimators': [75, 100, 200]}] #AdaBoost\n",
    "\n",
    "scores = ['precision', 'recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    \n",
    "    #base_estimator=DecisionTreeClassifier(min_samples_split=2, max_features='auto', max_depth=25, min_samples_leaf=1)\n",
    "    #base_estimator=ExtraTreesClassifier(max_depth=None, min_samples_split=2, n_jobs=4)\n",
    "    base_estimator=ExtraTreesClassifier(max_depth=None, class_weight='balanced',\n",
    "                                        min_samples_split=2, n_estimators=200, min_samples_leaf=16, n_jobs=4)\n",
    "    #base_estimator=RandomForestClassifier(max_depth=None, min_samples_split=2)\n",
    "    ada_real = AdaBoostClassifier(base_estimator=base_estimator, algorithm=\"SAMME.R\")\n",
    "\n",
    "    #clf = GridSearchCV(base_estimator, tuned_parameters, cv=5, scoring='%s_macro' % score)\n",
    "    clf = GridSearchCV(ada_real, tuned_parameters, cv=5, scoring='%s_macro' % score)\n",
    "    clf.fit(X_normalized, y_train)\n",
    "    #clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    #y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    y_true, y_pred = y_cv, clf.predict(X_cv_normalized)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75310045473336085"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_cv_normalized,y_cv) # Old parameters cv validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(bootstrap=False, class_weight='balanced',\n",
      "           criterion='gini', max_depth=None, max_features='auto',\n",
      "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "           min_samples_leaf=16, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=4,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "0.666666666667\n"
     ]
    }
   ],
   "source": [
    "ex = ExtraTreesClassifier(max_depth=None, min_samples_split=2, class_weight='balanced', \n",
    "                          min_samples_leaf=16, n_estimators=200, n_jobs=4)\n",
    "\n",
    "print(ex.fit(X_normalized, y_train))\n",
    "print()\n",
    "print(ex.score(X_test_normalized,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X, y = X_train, X_target\n",
    "indices = np.arange(y.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X, y = X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train scores \n",
      "\n",
      " [[ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]] \n",
      "\n",
      "Valid scores \n",
      "\n",
      " [[ 0.73584906  0.74213836  0.71698113  0.6918239   0.76582278]\n",
      " [ 0.72955975  0.72955975  0.72955975  0.71069182  0.75316456]\n",
      " [ 0.75471698  0.73584906  0.73584906  0.71698113  0.74683544]\n",
      " [ 0.7672956   0.73584906  0.72955975  0.71698113  0.75949367]\n",
      " [ 0.74842767  0.71698113  0.73584906  0.71069182  0.75949367]\n",
      " [ 0.74213836  0.73584906  0.73584906  0.70440252  0.76582278]]\n"
     ]
    }
   ],
   "source": [
    "param_range = [1,2,4,8,12,13]\n",
    "train_scores, test_scores = validation_curve(RandomForestClassifier(n_estimators=1000), X_train, X_target, param_name=\"max_features\", param_range=param_range,\n",
    "                                             cv=5, scoring=\"accuracy\", n_jobs=1)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "print(\"Train scores \\n\\n %s \\n\" % train_scores)\n",
    "print(\"Valid scores \\n\\n %s\" % test_scores)\n",
    "\n",
    "plt.title(\"Validation Curve with SVM\")\n",
    "plt.xlabel(\"$\\gamma$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "title = \"Learning Curves Random Forest Classifier 400 / 13\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = RandomForestClassifier(n_estimators=400, max_features=13, max_depth=None, min_samples_split=1)\n",
    "plot_learning_curve(estimator, title, X_train, X_target, ylim=(0.5, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "title = \"Learning Curves AdaBoost\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = AdaBoostClassifier(\n",
    "    base_estimator=RandomForestClassifier(max_depth=None, min_samples_split=1, n_estimators=100, max_features=12),\n",
    "    learning_rate=0.8,\n",
    "    n_estimators=600,\n",
    "    algorithm=\"SAMME.R\")\n",
    "plot_learning_curve(estimator, title, X_train, X_target, (0.5, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      42\n",
       "1      42\n",
       "2      43\n",
       "3      42\n",
       "4      43\n",
       "5      43\n",
       "6      43\n",
       "7      20\n",
       "8      20\n",
       "9      22\n",
       "10     20\n",
       "11     30\n",
       "12     13\n",
       "13     13\n",
       "14     13\n",
       "15     30\n",
       "16     30\n",
       "17     30\n",
       "18     13\n",
       "19     18\n",
       "20     18\n",
       "21      4\n",
       "22     18\n",
       "23     32\n",
       "24     32\n",
       "25     32\n",
       "26     32\n",
       "27     18\n",
       "28     18\n",
       "29     18\n",
       "       ..\n",
       "133    32\n",
       "134    32\n",
       "135    18\n",
       "136    18\n",
       "137    18\n",
       "138    13\n",
       "139    18\n",
       "140    13\n",
       "141    18\n",
       "142    18\n",
       "143    13\n",
       "144    18\n",
       "145    13\n",
       "146    18\n",
       "147    13\n",
       "148    27\n",
       "149    30\n",
       "150    30\n",
       "151    27\n",
       "152    30\n",
       "153    27\n",
       "154    27\n",
       "155    30\n",
       "156    27\n",
       "157    30\n",
       "158    27\n",
       "159    30\n",
       "160    27\n",
       "161    30\n",
       "162    30\n",
       "Name: PlayerName_Val, dtype: int32"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('C:\\Pedro\\workspace\\Acc-challenge\\\\test_data_v3.csv')\n",
    "#test = test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#t_pname_mapping = dict(zip(sorted(test['PlayerName'].unique()), range(0, len(sorted(test['PlayerName'].unique())) + 1)))\n",
    "#t_spos_mapping = dict(zip(sorted(test['StartingPositionDesc'].unique()), range(0, len(sorted(test['StartingPositionDesc'].unique())) + 1)))\n",
    "#t_epos_mapping = dict(zip(sorted(test['EndPositionDesc'].unique()), range(0, len(sorted(test['EndPositionDesc'].unique())) + 1)))\n",
    "#t_adesc_mapping = dict(zip(sorted(test['ActionTypeDesc'].unique()), range(0, len(sorted(test['ActionTypeDesc'].unique())) + 1)))\n",
    "#t_summary_mapping = dict(zip(sorted(test['Summary'].unique()), range(0, len(sorted(test['Summary'].unique())) + 1)))\n",
    "\n",
    "test['PlayerName'].apply(lambda x: pname_mapping.update(zip([x],[len(pname_mapping.keys())])) \n",
    "                         if pname_mapping.has_key(x) == False else None)\n",
    "\n",
    "test['StartingPositionDesc'].apply(lambda x: spos_mapping.update(zip([x],[len(spos_mapping.keys())])) \n",
    "                         if spos_mapping.has_key(x) == False else None)\n",
    "\n",
    "test['EndPositionDesc'].apply(lambda x: epos_mapping.update(zip([x],[len(epos_mapping.keys())])) \n",
    "                         if epos_mapping.has_key(x) == False else None)\n",
    "\n",
    "test['ActionTypeDesc'].apply(lambda x: atype_mapping.update(zip([x],[len(atype_mapping.keys())])) \n",
    "                         if atype_mapping.has_key(x) == False else None)\n",
    "\n",
    "test['Summary'].apply(lambda x: summary_mapping.update(zip([x],[len(summary_mapping.keys())])) \n",
    "                         if summary_mapping.has_key(x) == False else None)\n",
    "\n",
    "#test['PlayerName_Val'] = test['PlayerName'].map(t_pname_mapping).astype(int)\n",
    "#test['StartingPositionDesc_Val'] = test['StartingPositionDesc'].map(t_spos_mapping).astype(int)\n",
    "#test['ActionTypeDesc_Val'] = test['ActionTypeDesc'].map(t_adesc_mapping).astype(int)\n",
    "#test['EndPositionDesc_Val'] = test['EndPositionDesc'].map(t_epos_mapping).astype(int)\n",
    "#test['Summary_Val'] = test['Summary'].map(t_summary_mapping).astype(int)\n",
    "\n",
    "test['PlayerName_Val'] = test['PlayerName'].map(pname_mapping).astype(int)\n",
    "test['StartingPositionDesc_Val'] = test['StartingPositionDesc'].map(spos_mapping).astype(int)\n",
    "test['ActionTypeDesc_Val'] = test['ActionTypeDesc'].map(atype_mapping).astype(int)\n",
    "test['EndPositionDesc_Val'] = test['EndPositionDesc'].map(epos_mapping).astype(int)\n",
    "test['Summary_Val'] = test['Summary'].map(summary_mapping).astype(int)\n",
    "\n",
    "\n",
    "Test_ids = test['RecordID']\n",
    "Test_predict = test.drop(['RecordID','PlayerName','StartingPositionDesc',\n",
    "                              'EndPositionDesc','Summary','ActionTypeDesc'], axis=1).values\n",
    "\n",
    "#Test_predict = test.drop(['PlayerName','StartingPositionDesc','EndPositionDesc','Summary',\n",
    "#                             'Summary_Val','EndPositionDesc_Val','End_Y','ActionTypeDesc'], axis=1)\n",
    "\n",
    "# No need to shuffle prediction set (we are not fitting it)\n",
    "#np.random.seed(0)\n",
    "#Test_sorted = Test_predict.reindex(np.random.permutation(Test_predict.index))\n",
    "#X_test = Test_sorted.drop(['RecordID'], axis=1).values\n",
    "\n",
    "# Apply same transformation to test data (normalization)\n",
    "test_set_normalized = scaler.transform(Test_predict)\n",
    "test['PlayerName_Val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=12, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=1, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "          learning_rate=0.8, n_estimators=400, random_state=None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_real = AdaBoostClassifier(\n",
    "    base_estimator=RandomForestClassifier(max_depth=None, min_samples_split=1, n_estimators=100, max_features=12),\n",
    "    learning_rate=0.8,\n",
    "    n_estimators=400,\n",
    "    algorithm=\"SAMME.R\")\n",
    "    \n",
    "ada_real.fit(X_normalized, X_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1\n",
      " 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1\n",
      " 1 0 1 1 1 0 1 1 1 1 1]\n",
      "\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      " Goal Kick Missed       0.46      0.32      0.37        38\n",
      "Goal Kick Success       0.80      0.88      0.84       121\n",
      "\n",
      "      avg / total       0.72      0.75      0.73       159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_y = ada_real.predict(X_cv_normalized)\n",
    "print(pred_y)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "target_names = ['Goal Kick Missed', 'Goal Kick Success']\n",
    "print(classification_report(y_cv, pred_y, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1\n",
      " 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "pred_y_test = ada_real.predict(test_set_normalized)\n",
    "\n",
    "print(pred_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reverse_map = {0: 'Goal Kick Missed', 1: 'Goal Kick Success'}\n",
    "\n",
    "result = pd.DataFrame({'RowId': Test_ids})\n",
    "result['Prediction'] = pred_y_test.T\n",
    "result['Prediction'] = result['Prediction'].map(reverse_map)\n",
    "result\n",
    "\n",
    "#result.to_excel('PedroCastanha_AdaBoost_Recall_Normalized_NewMap.xlsx', index=False)\n",
    "result.to_excel('PedroCastanha_AdaBoost_Recall_Normalized_NewMap_v2.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 1 (0.127705)\n",
      "2. feature 0 (0.121489)\n",
      "3. feature 13 (0.079950)\n",
      "4. feature 11 (0.078692)\n",
      "5. feature 7 (0.077209)\n",
      "6. feature 9 (0.074318)\n",
      "7. feature 5 (0.069823)\n",
      "8. feature 8 (0.065933)\n",
      "9. feature 6 (0.064566)\n",
      "10. feature 2 (0.063508)\n",
      "11. feature 3 (0.050709)\n",
      "12. feature 4 (0.050206)\n",
      "13. feature 10 (0.048022)\n",
      "14. feature 12 (0.027871)\n"
     ]
    }
   ],
   "source": [
    "forest = ExtraTreesClassifier(n_estimators=250, random_state=0)\n",
    "forest.fit(X_train, X_target)\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), indices)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 794 entries, 460 to 684\n",
      "Data columns (total 14 columns):\n",
      "Start_X                     794 non-null int64\n",
      "Start_Y                     794 non-null int64\n",
      "End_X                       794 non-null int64\n",
      "Temperature                 794 non-null float64\n",
      "CloudCover                  794 non-null float64\n",
      "DewPoint                    794 non-null float64\n",
      "Humidity                    794 non-null float64\n",
      "Pressure                    794 non-null float64\n",
      "Visibility                  794 non-null float64\n",
      "WindDirection               794 non-null int64\n",
      "WindSpeed                   794 non-null float64\n",
      "PlayerName_Val              794 non-null int32\n",
      "ActionType_Val              794 non-null int32\n",
      "StartingPositionDesc_Val    794 non-null int32\n",
      "dtypes: float64(7), int32(3), int64(4)\n",
      "memory usage: 83.7 KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
