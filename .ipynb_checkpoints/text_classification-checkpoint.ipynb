{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define default labels\n",
    "labels = ['Fatos', 'Normas', 'Argumentos', 'Pedidos', 'Irrelevante']\n",
    "\n",
    "# Define stopwords list\n",
    "stopwords = []\n",
    "\n",
    "raw_data = pd.read_csv('C:\\Users\\pedro.castanha\\Downloads\\ML_Gabinete_Digital.csv', error_bad_lines=False, sep='\\t', encoding='utf_8')\n",
    "words = pd.read_table('C:\\Users\\pedro.castanha\\Downloads\\stoplists\\stopwords_pt_br.txt', encoding='mbcs')\n",
    "\n",
    "stopwords = words.values.T.tolist()[0]\n",
    "\n",
    "x_vectorized = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words=stopwords)\n",
    "x_vectorized.fit(raw_data.Text)\n",
    "x_train = x_vectorized.transform(raw_data.Text)\n",
    "y_train = raw_data.Class\n",
    "\n",
    "names = np.asarray(x_vectorized.get_feature_names())\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load some categories from the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
      "data loaded\n",
      "2034 documents - 3.980MB (training set)\n",
      "1353 documents - 2.867MB (test set)\n",
      "4 categories\n",
      "\n",
      "Extracting features from the training data using a sparse vectorizer\n",
      "done in 0.805000s at 4.944MB/s\n",
      "n_samples: 2034, n_features: 33809\n",
      "\n",
      "Extracting features from the test data using the same vectorizer\n",
      "done in 1.266000s at 2.265MB/s\n",
      "n_samples: 1353, n_features: 33809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#if opts.all_categories:\n",
    "#    categories = None\n",
    "\n",
    "categories = [\n",
    "        'alt.atheism',\n",
    "        'talk.religion.misc',\n",
    "        'comp.graphics',\n",
    "        'sci.space',\n",
    "]\n",
    "\n",
    "#if opts.filtered:\n",
    "#    remove = ('headers', 'footers', 'quotes')\n",
    "#else:\n",
    "remove = ()\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories if categories else \"all\")\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "print('data loaded')\n",
    "\n",
    "# order of labels in `target_names` can be different from `categories`\n",
    "target_names = data_train.target_names\n",
    "\n",
    "\n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
    "\n",
    "data_train_size_mb = size_mb(data_train.data)\n",
    "data_test_size_mb = size_mb(data_test.data)\n",
    "\n",
    "print(\"%d documents - %0.3fMB (training set)\" % (\n",
    "    len(data_train.data), data_train_size_mb))\n",
    "print(\"%d documents - %0.3fMB (test set)\" % (\n",
    "    len(data_test.data), data_test_size_mb))\n",
    "print(\"%d categories\" % len(categories))\n",
    "print()\n",
    "\n",
    "# split a training set and a test set\n",
    "y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "#if opts.use_hashing:\n",
    "#    vectorizer = HashingVectorizer(stop_words='english', non_negative=True,\n",
    "#                                   n_features=opts.n_features)\n",
    "#    X_train = vectorizer.transform(data_train.data)\n",
    "#else:\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                 stop_words='english')\n",
    "X_train = vectorizer.fit_transform(data_train.data)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "print()\n",
    "\n",
    "print(\"Extracting features from the test data using the same vectorizer\")\n",
    "t0 = time()\n",
    "X_test = vectorizer.transform(data_test.data)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_test_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "print()\n",
    "\n",
    "# mapping from integer feature name to original token string\n",
    "#if opts.use_hashing:\n",
    "#    feature_names = None\n",
    "#else:\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "#if opts.select_chi2:\n",
    "#    print(\"Extracting %d best features by a chi-squared test\" %\n",
    "#          opts.select_chi2)\n",
    "#    t0 = time()\n",
    "#    ch2 = SelectKBest(chi2, k=opts.select_chi2)\n",
    "#    X_train = ch2.fit_transform(X_train, y_train)\n",
    "#    X_test = ch2.transform(X_test)\n",
    "#    if feature_names:\n",
    "        # keep selected feature names\n",
    "#        feature_names = [feature_names[i] for i\n",
    "#                         in ch2.get_support(indices=True)]\n",
    "#    print(\"done in %fs\" % (time() - t0))\n",
    "#    print()\n",
    "\n",
    "if feature_names:\n",
    "    feature_names = np.asarray(feature_names)\n",
    "\n",
    "\n",
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projecting the input data...\n"
     ]
    }
   ],
   "source": [
    "# Init - PCA Dim. reduction\n",
    "print(\"Projecting the input data...\")\n",
    "svd = TruncatedSVD(n_components=300, algorithm='randomized',).fit(X_train.toarray())\n",
    "X_train = svd.transform(X_train)\n",
    "X_test = svd.transform(X_test)\n",
    "\n",
    "tsne = TSNE(n_components=20, init='pca', random_state=0)\n",
    "X_train = tsne.fit_transform(X_train)\n",
    "X_test = tsne.transform(X_test)\n",
    "print(\"Done\")\n",
    "# End - PCA Dim. reduction\n",
    "\n",
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()    \n",
    "    clf.fit(X_train, y_train)\n",
    "    #clf.fit(x_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    #pred = clf.predict(x_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "        if True and feature_names is not None:\n",
    "        #if True and names is not None:\n",
    "            print(\"top 10 keywords per class:\")\n",
    "            for i, label in enumerate(target_names):\n",
    "            #for i, label in enumerate(labels):\n",
    "                top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "                print(trim(\"%s: %s\" % (label, \" \".join(feature_names[top10]))))\n",
    "                #print(trim(\"%s: %s\" % (label, \" \".join(names[top10]))))\n",
    "        print()\n",
    "\n",
    "    if True:\n",
    "        print(\"classification report:\")\n",
    "        print(metrics.classification_report(y_test, pred,\n",
    "                                            target_names=target_names))\n",
    "\n",
    "    if True:\n",
    "        print(\"confusion matrix:\")\n",
    "        print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time\n",
    "\n",
    "\n",
    "results = []\n",
    "for clf, name in (\n",
    "        (RidgeClassifier(tol=1e-2, solver=\"lsqr\"), \"Ridge Classifier\"),\n",
    "        (Perceptron(n_iter=50), \"Perceptron\"),\n",
    "        (PassiveAggressiveClassifier(n_iter=50), \"Passive-Aggressive\"),\n",
    "        (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "        (RandomForestClassifier(n_estimators=100), \"Random forest\")):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))\n",
    "\n",
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    print('=' * 80)\n",
    "    print(\"%s penalty\" % penalty.upper())\n",
    "    # Train Liblinear model\n",
    "    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,\n",
    "                                            dual=False, tol=1e-3)))\n",
    "\n",
    "    # Train SGD model\n",
    "    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                           penalty=penalty)))\n",
    "\n",
    "# Train SGD with Elastic Net penalty\n",
    "print('=' * 80)\n",
    "print(\"Elastic-Net penalty\")\n",
    "results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                       penalty=\"elasticnet\")))\n",
    "\n",
    "# Train NearestCentroid without threshold\n",
    "print('=' * 80)\n",
    "print(\"NearestCentroid (aka Rocchio classifier)\")\n",
    "results.append(benchmark(NearestCentroid()))\n",
    "\n",
    "# Train sparse Naive Bayes classifiers\n",
    "print('=' * 80)\n",
    "print(\"Naive Bayes\")\n",
    "#results.append(benchmark(MultinomialNB(alpha=.01)))\n",
    "#results.append(benchmark(BernoulliNB(alpha=.01)))\n",
    "\n",
    "print('=' * 80)\n",
    "print(\"LinearSVC with L1-based feature selection\")\n",
    "# The smaller C, the stronger the regularization.\n",
    "# The more regularization, the more sparsity.\n",
    "results.append(benchmark(Pipeline([\n",
    "  ('feature_selection', LinearSVC(penalty=\"l1\", dual=False, tol=1e-3)),\n",
    "  ('classification', LinearSVC())\n",
    "])))\n",
    "\n",
    "# make some plots\n",
    "\n",
    "indices = np.arange(len(results))\n",
    "\n",
    "results = [[x[i] for x in results] for i in range(4)]\n",
    "\n",
    "clf_names, score, training_time, test_time = results\n",
    "training_time = np.array(training_time) / np.max(training_time)\n",
    "test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Score\")\n",
    "plt.barh(indices, score, .2, label=\"score\", color='navy')\n",
    "plt.barh(indices + .3, training_time, .2, label=\"training time\",\n",
    "         color='c')\n",
    "plt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\n",
    "plt.yticks(())\n",
    "plt.legend(loc='best')\n",
    "plt.subplots_adjust(left=.25)\n",
    "plt.subplots_adjust(top=.95)\n",
    "plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "for i, c in zip(indices, clf_names):\n",
    "    plt.text(-.3, i, c)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      Fatos       0.73      0.68      0.70        47\n",
      "     Normas       0.70      0.72      0.71        43\n",
      " Argumentos       0.43      0.52      0.47        44\n",
      "    Pedidos       0.55      0.71      0.62        34\n",
      "Irrelevante       0.76      0.60      0.67        85\n",
      "\n",
      "avg / total       0.66      0.64      0.64       253\n",
      "\n",
      "Fatos: nº réu reclamante anexo qualquer 00 data imóvel conforme requerente\n",
      "Normas: ser qualquer atos ato processual processo iii lei ii art\n",
      "Argumentos: protesto réu ré razão requerente requerida imóvel assim autor ser\n",
      "Pedidos: advocatícios custas bem 20 honorários juros processuais pagamento co...\n",
      "Irrelevante: tutela direito contrato moral mais posse exposto dano ação pedido\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv('C:\\Users\\pedro.castanha\\Downloads\\ML_Gabinete_Digital.csv', error_bad_lines=False, sep='\\t', encoding='utf_8')\n",
    "words = pd.read_table('C:\\Users\\pedro.castanha\\Downloads\\stoplists\\stopwords_pt_br.txt', encoding='mbcs')\n",
    "\n",
    "stopwords = words.values.T.tolist()[0]\n",
    "\n",
    "x_vectorized = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words=stopwords)\n",
    "x_vectorized.fit(raw_data.Text)\n",
    "x_train = x_vectorized.transform(raw_data.Text)\n",
    "y_train = raw_data.Class\n",
    "\n",
    "names = np.asarray(x_vectorized.get_feature_names())\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.4, random_state=42)\n",
    "\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "    print(trim(\"%s: %s\" % (label, \" \".join(names[top10]))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
