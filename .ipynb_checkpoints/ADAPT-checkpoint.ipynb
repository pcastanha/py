{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:\\Pedro\\workspace\\Acc-challenge\\data.csv')\n",
    "train = train.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timezone_mapping = dict(zip(sorted(train['Timezone'].unique()), range(0, len(sorted(train['Timezone'].unique())) + 1)))\n",
    "fixture_mapping = dict(zip(sorted(train['FixtureDate'].unique()), range(0, len(sorted(train['FixtureDate'].unique())) + 1)))\n",
    "hteam_mapping = dict(zip(sorted(train['HomeTeam'].unique()), range(0, len(sorted(train['HomeTeam'].unique())) + 1)))\n",
    "ateam_mapping = dict(zip(sorted(train['AwayTeam'].unique()), range(0, len(sorted(train['AwayTeam'].unique())) + 1)))\n",
    "team_mapping = dict(zip(sorted(train['Team'].unique()), range(0, len(sorted(train['Team'].unique())) + 1)))\n",
    "atype_mapping = dict(zip(sorted(train['ActionTypeDesc'].unique()), range(0, len(sorted(train['ActionTypeDesc'].unique())) + 1)))\n",
    "res_mapping = dict(zip(sorted(train['ActionResultDesc'].unique()), range(0, len(sorted(train['ActionResultDesc'].unique())) + 1)))\n",
    "start_mapping = dict(zip(sorted(train['StartingPositionDesc'].unique()), range(0, len(sorted(train['StartingPositionDesc'].unique())) + 1)))\n",
    "end_mapping = dict(zip(sorted(train['EndPositionDesc'].unique()), range(0, len(sorted(train['EndPositionDesc'].unique())) + 1)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['Timezone_Val'] = train['Timezone'].map(timezone_mapping).astype(int)\n",
    "train['Fixture_Val'] = train['FixtureDate'].map(fixture_mapping).astype(int)\n",
    "train['Hteam_Val'] = train['HomeTeam'].map(hteam_mapping).astype(int)\n",
    "train['Ateam_Val'] = train['AwayTeam'].map(ateam_mapping).astype(int)\n",
    "train['Team_Val'] = train['Team'].map(team_mapping).astype(int)\n",
    "train['Atype_Val'] = train['ActionTypeDesc'].map(atype_mapping).astype(int)\n",
    "train['Start_Val'] = train['StartingPositionDesc'].map(start_mapping).astype(int)\n",
    "train['End_Val'] = train['EndPositionDesc'].map(end_mapping).astype(int)\n",
    "train['Res_Val'] = train['ActionResultDesc'].map(res_mapping).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train.dtypes[train.dtypes.map(lambda x: x == 'object')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test = train.drop(['LineOutAttackers','FixtureDate','StadiumName','KickOffTime_GMT','Timezone','KickOffTime_Local','HomeTeam',\n",
    "                        'AwayTeam','Team','ActionTypeDesc','ActionResultDesc','StartingPositionDesc',\n",
    "                        'EndPositionDesc','PlayDirection','City','Icon','Summary','ZoneDescription','CloudCover','Pressure'],\n",
    "                        axis=1)\n",
    "\n",
    "train_values = train_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "train_features = train_values[:634, 1:48]\n",
    "cv_features = train_values[634:,1:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_target = train_test['Res_Val'].values[:634]\n",
    "cv_target = train_test['Res_Val'].values[634:]\n",
    "#clf = clf.fit(train_features, train_target)\n",
    "#score = clf.score(train_features, train_target)\n",
    "#\"Mean accuracy of Random Forest: {0}\".format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=0.00414094,\n",
       "  kernel='linear', max_iter=-1, probability=True, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='linear', C=1.0, probability=True, gamma=0.00414094)\n",
    "svc\n",
    "#svc = svc.fit(train_features, train_target)\n",
    "#svc.score(train_features, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#svm.predict(train_test.iloc[:6,:48].values)\n",
    "#res = svc.predict(cv_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('C:\\Pedro\\workspace\\Acc-challenge\\\\test_data.csv')\n",
    "test = test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "t_timezone_mapping = dict(zip(sorted(test['Timezone'].unique()), range(0, len(sorted(test['Timezone'].unique())) + 1)))\n",
    "t_fixture_mapping = dict(zip(sorted(test['FixtureDate'].unique()), range(0, len(sorted(test['FixtureDate'].unique())) + 1)))\n",
    "t_hteam_mapping = dict(zip(sorted(test['HomeTeam'].unique()), range(0, len(sorted(test['HomeTeam'].unique())) + 1)))\n",
    "t_ateam_mapping = dict(zip(sorted(test['AwayTeam'].unique()), range(0, len(sorted(test['AwayTeam'].unique())) + 1)))\n",
    "t_team_mapping = dict(zip(sorted(test['Team'].unique()), range(0, len(sorted(test['Team'].unique())) + 1)))\n",
    "t_atype_mapping = dict(zip(sorted(test['ActionTypeDesc'].unique()), range(0, len(sorted(test['ActionTypeDesc'].unique())) + 1)))\n",
    "t_res_mapping = dict(zip(sorted(test['ActionResultDesc'].unique()), range(0, len(sorted(test['ActionResultDesc'].unique())) + 1)))\n",
    "t_start_mapping = dict(zip(sorted(test['StartingPositionDesc'].unique()), range(0, len(sorted(test['StartingPositionDesc'].unique())) + 1)))\n",
    "t_end_mapping = dict(zip(sorted(test['EndPositionDesc'].unique()), range(0, len(sorted(test['EndPositionDesc'].unique())) + 1)))\n",
    "\n",
    "test['Timezone_Val'] = test['Timezone'].map(t_timezone_mapping).astype(int)\n",
    "test['Fixture_Val'] = test['FixtureDate'].map(t_fixture_mapping).astype(int)\n",
    "test['Hteam_Val'] = test['HomeTeam'].map(t_hteam_mapping).astype(int)\n",
    "test['Ateam_Val'] = test['AwayTeam'].map(t_ateam_mapping).astype(int)\n",
    "test['Team_Val'] = test['Team'].map(t_team_mapping).astype(int)\n",
    "test['Atype_Val'] = test['ActionTypeDesc'].map(t_atype_mapping).astype(int)\n",
    "test['Start_Val'] = test['StartingPositionDesc'].map(t_start_mapping).astype(int)\n",
    "test['End_Val'] = test['EndPositionDesc'].map(t_end_mapping).astype(int)\n",
    "test['Res_Val'] = test['ActionResultDesc'].map(t_res_mapping).astype(int)\n",
    "\n",
    "test_set = test.drop(['FixtureDate','StadiumName','KickOffTime_GMT','Timezone','KickOffTime_Local','HomeTeam',\n",
    "                        'AwayTeam','Team','ActionTypeDesc','ActionResultDesc','StartingPositionDesc','EndPositionDesc',\n",
    "                        'PlayDirection','City','Icon','Summary','ZoneDescription','CloudCover','Pressure','Visibility'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_features = test_set.values[:, 1:48]\n",
    "test_res = test_set['Res_Val'].values\n",
    "#test_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_features)\n",
    "X_train = scaler.transform(train_features)\n",
    "#X_cv = scaler.transform(cv_features)\n",
    "X_test = scaler.transform(test_features)  # apply same transformation to test data\n",
    "\n",
    "svc = svc.fit(X_train,train_target)\n",
    "predict_y = svc.predict(X_test)\n",
    "predict_y\n",
    "\n",
    "#sgdc = SGDClassifier(loss=\"log\", penalty=\"elasticnet\", n_iter=np.ceil(10**6 / 600))\n",
    "#sgdc = sgdc.fit(train_features, train_target)\n",
    "#sgdc.score(train_features, train_target)\n",
    "#predict_y = sgdc.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      " Not Scored       1.00      1.00      1.00       125\n",
      "     Scored       1.00      1.00      1.00        35\n",
      "\n",
      "avg / total       1.00      1.00      1.00       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predict_cv = sgdc.predict(cv_features)\n",
    "#print(classification_report(cv_target, predict_cv, target_names=['Not Scored', 'Scored']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix  [[129   0]\n",
      " [  0  34]]\n",
      "          Predicted\n",
      "         |  0  |  1  |\n",
      "         |-----|-----|\n",
      "       0 | 129 |   0 |\n",
      "Actual   |-----|-----|\n",
      "       1 |   0 |  34 |\n",
      "         |-----|-----|\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " Not Scored       1.00      1.00      1.00       129\n",
      "     Scored       1.00      1.00      1.00        34\n",
      "\n",
      "avg / total       1.00      1.00      1.00       163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(test_res, predict_y)\n",
    "\n",
    "print (\"Confusion Matrix \", confusion_matrix)\n",
    "\n",
    "print (\"          Predicted\")\n",
    "print (\"         |  0  |  1  |\")\n",
    "print (\"         |-----|-----|\")\n",
    "print (\"       0 | %3d | %3d |\" % (confusion_matrix[0, 0],\n",
    "                                   confusion_matrix[0, 1]))\n",
    "print (\"Actual   |-----|-----|\")\n",
    "print (\"       1 | %3d | %3d |\" % (confusion_matrix[1, 0],\n",
    "                                   confusion_matrix[1, 1]))\n",
    "print (\"         |-----|-----|\")\n",
    "\n",
    "print(classification_report(test_res, predict_y, target_names=['Not Scored', 'Scored']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_probs = svc.predict_proba(X_test)\n",
    "#metrics.log_loss(test_res.ravel(), test_probs[:,1])\n",
    "#metrics.roc_auc_score(test_res, test_probs[:,1])\n",
    "\n",
    "result = pd.DataFrame({'RecordID': test_set['RecordID']})\n",
    "result['ActionName'] = test_res.T\n",
    "result = result.replace(to_replace={0,1}, value={'Goal Kick Success', 'Goal Kick Missed'})\n",
    "#result.to_excel('PedroCastanha.xlsx', index=False)\n",
    "#print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 10 (0.661703)\n",
      "2. feature 11 (0.031892)\n",
      "3. feature 17 (0.031869)\n",
      "4. feature 12 (0.025410)\n",
      "5. feature 35 (0.011735)\n",
      "6. feature 34 (0.011574)\n",
      "7. feature 6 (0.011490)\n",
      "8. feature 8 (0.011054)\n",
      "9. feature 5 (0.010814)\n",
      "10. feature 9 (0.009994)\n",
      "11. feature 14 (0.009298)\n",
      "12. feature 13 (0.009138)\n",
      "13. feature 46 (0.009005)\n",
      "14. feature 41 (0.008473)\n",
      "15. feature 36 (0.008166)\n",
      "16. feature 39 (0.007933)\n",
      "17. feature 38 (0.007698)\n",
      "18. feature 7 (0.007553)\n",
      "19. feature 37 (0.006907)\n",
      "20. feature 18 (0.006611)\n",
      "21. feature 19 (0.006031)\n",
      "22. feature 43 (0.005907)\n",
      "23. feature 1 (0.005848)\n",
      "24. feature 44 (0.005641)\n",
      "25. feature 45 (0.005428)\n",
      "26. feature 2 (0.005380)\n",
      "27. feature 22 (0.005337)\n",
      "28. feature 23 (0.005304)\n",
      "29. feature 3 (0.005128)\n",
      "30. feature 24 (0.004815)\n",
      "31. feature 4 (0.004716)\n",
      "32. feature 16 (0.004697)\n",
      "33. feature 0 (0.004665)\n",
      "34. feature 40 (0.004628)\n",
      "35. feature 25 (0.004419)\n",
      "36. feature 20 (0.004145)\n",
      "37. feature 21 (0.003488)\n",
      "38. feature 42 (0.003454)\n",
      "39. feature 33 (0.002173)\n",
      "40. feature 27 (0.002157)\n",
      "41. feature 29 (0.002055)\n",
      "42. feature 30 (0.001858)\n",
      "43. feature 26 (0.001834)\n",
      "44. feature 15 (0.001417)\n",
      "45. feature 28 (0.001158)\n",
      "46. feature 31 (0.000000)\n",
      "47. feature 32 (0.000000)\n"
     ]
    }
   ],
   "source": [
    "# Build a forest and compute the feature importances\n",
    "#forest = ExtraTreesClassifier(n_estimators=250, random_state=0)\n",
    "#forest.fit(X_train, train_target)\n",
    "\n",
    "#importances = forest.feature_importances_\n",
    "#std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "#indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "#print(\"Feature ranking:\")\n",
    "\n",
    "#for f in range(X_train.shape[1]):\n",
    "#    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "#plt.figure()\n",
    "#plt.title(\"Feature importances\")\n",
    "#plt.bar(range(X_train.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n",
    "#plt.xticks(range(X_train.shape[1]), indices)\n",
    "#plt.xlim([-1, X_train.shape[1]])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_train2 = train_test.iloc[:634,[10,11,17,12,35,34,6,8,5,9]]\n",
    "X_train2 = train_test.iloc[:634,[10,11]]\n",
    "X_train2 = X_train2.values\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train2)\n",
    "X_train2 = scaler.transform(X_train2)\n",
    "#svc = svc.fit(X_train2,train_target)\n",
    "clf = clf.fit(X_train2,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Confusion Matrix ', array([[129,   0],\n",
      "       [  0,  34]]))\n",
      "          Predicted\n",
      "         |  0  |  1  |\n",
      "         |-----|-----|\n",
      "       0 | 129 |   0 |\n",
      "Actual   |-----|-----|\n",
      "       1 |   0 |  34 |\n",
      "         |-----|-----|\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     Scored       1.00      1.00      1.00       129\n",
      " Not Scored       1.00      1.00      1.00        34\n",
      "\n",
      "avg / total       1.00      1.00      1.00       163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test = scaler.transform(test_set.iloc[:634,[10,11]].values)\n",
    "#predict_y = svc.predict(X_test)\n",
    "predict_y = clf.predict(X_test)\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(test_res, predict_y)\n",
    "\n",
    "print (\"Confusion Matrix \", confusion_matrix)\n",
    "\n",
    "print (\"          Predicted\")\n",
    "print (\"         |  0  |  1  |\")\n",
    "print (\"         |-----|-----|\")\n",
    "print (\"       0 | %3d | %3d |\" % (confusion_matrix[0, 0],\n",
    "                                   confusion_matrix[0, 1]))\n",
    "print (\"Actual   |-----|-----|\")\n",
    "print (\"       1 | %3d | %3d |\" % (confusion_matrix[1, 0],\n",
    "                                   confusion_matrix[1, 1]))\n",
    "print (\"         |-----|-----|\")\n",
    "\n",
    "print(classification_report(test_res, predict_y, target_names=['Scored', 'Not Scored']))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
