{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:\\Pedro\\workspace\\Acc-challenge\\data_v3_3.csv')\n",
    "#train = pd.read_csv('C:\\Pedro\\workspace\\Acc-challenge\\data_v3_2.csv')\n",
    "#test = pd.read_csv('C:\\Pedro\\workspace\\Acc-challenge\\\\test_data_v3.csv')\n",
    "#train = train.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#concated_names = train['PlayerName'].append(test['PlayerName']).unique()\n",
    "#pname_mapping = dict(zip(sorted(concated_names), range(0, len(sorted(concated_names)) + 1)))\n",
    "pname_mapping = dict(zip(sorted(train['PlayerName'].unique()), range(0, len(sorted(train['PlayerName'].unique())) + 1)))\n",
    "aname_mapping = dict(zip(sorted(train['ActionName'].unique()), range(0, len(sorted(train['ActionName'].unique())) + 1)))\n",
    "atype_mapping = dict(zip(sorted(train['ActionTypeDesc'].unique()), range(0, len(sorted(train['ActionTypeDesc'].unique())) + 1)))\n",
    "spos_mapping = dict(zip(sorted(train['StartingPositionDesc'].unique()), range(0, len(sorted(train['StartingPositionDesc'].unique())) + 1)))\n",
    "epos_mapping = dict(zip(sorted(train['EndPositionDesc'].unique()), range(0, len(sorted(train['EndPositionDesc'].unique())) + 1)))\n",
    "stad_mapping = dict(zip(sorted(train['StadiumName'].unique()), range(0, len(sorted(train['StadiumName'].unique())) + 1)))\n",
    "team_mapping = dict(zip(sorted(train['Team'].unique()), range(0, len(sorted(train['Team'].unique())) + 1)))\n",
    "zone_mapping = dict(zip(sorted(train['ZoneDescription'].unique()), range(0, len(sorted(train['ZoneDescription'].unique())) + 1)))\n",
    "summary_mapping = dict(zip(sorted(train['Summary'].unique()), range(0, len(sorted(train['Summary'].unique())) + 1)))\n",
    "\n",
    "train['HomeTeam'].apply(lambda x: team_mapping.update(zip([x],[len(team_mapping.keys())])) \n",
    "                         if team_mapping.has_key(x) == False else None)\n",
    "\n",
    "train['AwayTeam'].apply(lambda x: team_mapping.update(zip([x],[len(team_mapping.keys())])) \n",
    "                         if team_mapping.has_key(x) == False else None)\n",
    "\n",
    "train['PlayerName_Val'] = train['PlayerName'].map(pname_mapping).astype(int)\n",
    "train['ActionName_Val'] = train['ActionName'].map(aname_mapping).astype(int)\n",
    "train['ActionType_Val'] = train['ActionTypeDesc'].map(atype_mapping).astype(int)\n",
    "train['StartingPositionDesc_Val'] = train['StartingPositionDesc'].map(spos_mapping).astype(int)\n",
    "train['EndPositionDesc_Val'] = train['EndPositionDesc'].map(epos_mapping).astype(int)\n",
    "train['StadiumName_Val'] = train['StadiumName'].map(stad_mapping).astype(int)\n",
    "train['Team_Val'] = train['Team'].map(team_mapping).astype(int)\n",
    "train['HomeTeam_Val'] = train['HomeTeam'].map(team_mapping).astype(int)\n",
    "train['AwayTeam_Val'] = train['AwayTeam'].map(team_mapping).astype(int)\n",
    "train['ZoneDescription_Val'] = train['ZoneDescription'].map(zone_mapping).astype(int)\n",
    "train['Summary_Val'] = train['Summary'].map(summary_mapping).astype(int)\n",
    "\n",
    "np.random.seed(0)\n",
    "train = train.reindex(np.random.permutation(train.index)) #Shuffling\n",
    "\n",
    "X_target = train['ActionName_Val'].values\n",
    "\n",
    "X_train = train.drop(['ActionName_Val','ActionName','PlayerName','StartingPositionDesc',\n",
    "                      'EndPositionDesc','Summary','RecordID','ActionTypeDesc','StadiumName',\n",
    "                        'Team','HomeTeam','AwayTeam','ZoneDescription','Visibility','Pressure',\n",
    "                         'Humidity','DewPoint','CloudCover','Temperature'], axis=1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Splitting data into train, cv and test sets using 60%, 20%, 20% proportions\n",
    "#X_train, X_cv, y_train, y_cv = train_test_split(X_train, X_target, test_size=0.4, random_state=42)\n",
    "#X_cv, X_test, y_cv, y_test = train_test_split(X_cv, y_cv, test_size=0.5, random_state=42)\n",
    "\n",
    "# Splitting data into train, cv and test sets using 80%, 10%, 10% proportions\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, X_target, test_size=0.2, random_state=42)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X_cv, y_cv, test_size=0.5, random_state=42)\n",
    "\n",
    "# Using only Train and CV: 80-20\n",
    "#X_train, X_cv, y_train, y_cv = train_test_split(X_train, X_target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.015625    0.0345028   0.07618835  0.16823752  0.37149857  0.82033536\n",
      "  1.81144733  4.        ]\n",
      "[10, 40, 70, 100, 130, 160, 190]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_normalized = scaler.transform(X_train)\n",
    "#X_cv_normalized = scaler.transform(X_cv)\n",
    "#X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "\n",
    "#rate_param = np.logspace(-3,1,num=4,base=2.0)\n",
    "rate_param = np.logspace(-6,2,8,base=2.0)\n",
    "n_est = range(10, 200, 30)\n",
    "print(rate_param)\n",
    "print(n_est)\n",
    "\n",
    "#tuned_parameters = [{'max_features': [4,6,8,10,12,14,16,20,22,24,'auto'], \n",
    "#                     'n_estimators': [10, 25, 50, 75, 100], 'class_weight': ['balanced', None]}] #RandomForest\n",
    "\n",
    "#tuned_parameters = [{'min_samples_leaf': range(1, 70, 5), \n",
    "#                     'min_samples_split': range(2, 20, 2)}] #RandomForest\n",
    "\n",
    "#tuned_parameters = [{'learning_rate': [1.4,1.6,1.2,1.0,0.8,0.6,0.4],\n",
    "#                     'n_estimators': [10, 25, 50, 75, 100, 200, 400, 600]}] #AdaBoost\n",
    "\n",
    "#tuned_parameters = [{'learning_rate': [0.31498026],'n_estimators': [200]}] #AdaBoost\n",
    "\n",
    "tuned_parameters = [{'learning_rate': rate_param,'n_estimators': n_est}] #AdaBoost\n",
    "\n",
    "scores = ['precision', 'recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_estimators': 70, 'learning_rate': 0.034502797302306626}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.864 (+/-0.063) for {'n_estimators': 10, 'learning_rate': 0.015625}\n",
      "0.858 (+/-0.057) for {'n_estimators': 40, 'learning_rate': 0.015625}\n",
      "0.815 (+/-0.071) for {'n_estimators': 70, 'learning_rate': 0.015625}\n",
      "0.828 (+/-0.096) for {'n_estimators': 100, 'learning_rate': 0.015625}\n",
      "0.843 (+/-0.089) for {'n_estimators': 130, 'learning_rate': 0.015625}\n",
      "0.842 (+/-0.081) for {'n_estimators': 160, 'learning_rate': 0.015625}\n",
      "0.834 (+/-0.058) for {'n_estimators': 190, 'learning_rate': 0.015625}\n",
      "0.835 (+/-0.068) for {'n_estimators': 10, 'learning_rate': 0.034502797302306626}\n",
      "0.836 (+/-0.091) for {'n_estimators': 40, 'learning_rate': 0.034502797302306626}\n",
      "0.865 (+/-0.055) for {'n_estimators': 70, 'learning_rate': 0.034502797302306626}\n",
      "0.856 (+/-0.074) for {'n_estimators': 100, 'learning_rate': 0.034502797302306626}\n",
      "0.832 (+/-0.073) for {'n_estimators': 130, 'learning_rate': 0.034502797302306626}\n",
      "0.836 (+/-0.075) for {'n_estimators': 160, 'learning_rate': 0.034502797302306626}\n",
      "0.847 (+/-0.071) for {'n_estimators': 190, 'learning_rate': 0.034502797302306626}\n",
      "0.825 (+/-0.110) for {'n_estimators': 10, 'learning_rate': 0.076188353387779711}\n",
      "0.839 (+/-0.054) for {'n_estimators': 40, 'learning_rate': 0.076188353387779711}\n",
      "0.836 (+/-0.094) for {'n_estimators': 70, 'learning_rate': 0.076188353387779711}\n",
      "0.843 (+/-0.106) for {'n_estimators': 100, 'learning_rate': 0.076188353387779711}\n",
      "0.851 (+/-0.093) for {'n_estimators': 130, 'learning_rate': 0.076188353387779711}\n",
      "0.836 (+/-0.089) for {'n_estimators': 160, 'learning_rate': 0.076188353387779711}\n",
      "0.852 (+/-0.075) for {'n_estimators': 190, 'learning_rate': 0.076188353387779711}\n",
      "0.843 (+/-0.109) for {'n_estimators': 10, 'learning_rate': 0.16823752407904449}\n",
      "0.838 (+/-0.066) for {'n_estimators': 40, 'learning_rate': 0.16823752407904449}\n",
      "0.857 (+/-0.098) for {'n_estimators': 70, 'learning_rate': 0.16823752407904449}\n",
      "0.841 (+/-0.104) for {'n_estimators': 100, 'learning_rate': 0.16823752407904449}\n",
      "0.837 (+/-0.095) for {'n_estimators': 130, 'learning_rate': 0.16823752407904449}\n",
      "0.835 (+/-0.102) for {'n_estimators': 160, 'learning_rate': 0.16823752407904449}\n",
      "0.845 (+/-0.088) for {'n_estimators': 190, 'learning_rate': 0.16823752407904449}\n",
      "0.841 (+/-0.075) for {'n_estimators': 10, 'learning_rate': 0.37149857228423705}\n",
      "0.847 (+/-0.088) for {'n_estimators': 40, 'learning_rate': 0.37149857228423705}\n",
      "0.844 (+/-0.097) for {'n_estimators': 70, 'learning_rate': 0.37149857228423705}\n",
      "0.850 (+/-0.087) for {'n_estimators': 100, 'learning_rate': 0.37149857228423705}\n",
      "0.861 (+/-0.099) for {'n_estimators': 130, 'learning_rate': 0.37149857228423705}\n",
      "0.847 (+/-0.069) for {'n_estimators': 160, 'learning_rate': 0.37149857228423705}\n",
      "0.842 (+/-0.043) for {'n_estimators': 190, 'learning_rate': 0.37149857228423705}\n",
      "0.856 (+/-0.079) for {'n_estimators': 10, 'learning_rate': 0.82033535600763752}\n",
      "0.851 (+/-0.059) for {'n_estimators': 40, 'learning_rate': 0.82033535600763752}\n",
      "0.852 (+/-0.060) for {'n_estimators': 70, 'learning_rate': 0.82033535600763752}\n",
      "0.844 (+/-0.061) for {'n_estimators': 100, 'learning_rate': 0.82033535600763752}\n",
      "0.838 (+/-0.095) for {'n_estimators': 130, 'learning_rate': 0.82033535600763752}\n",
      "0.843 (+/-0.062) for {'n_estimators': 160, 'learning_rate': 0.82033535600763752}\n",
      "0.830 (+/-0.062) for {'n_estimators': 190, 'learning_rate': 0.82033535600763752}\n",
      "0.842 (+/-0.049) for {'n_estimators': 10, 'learning_rate': 1.8114473285278128}\n",
      "0.853 (+/-0.067) for {'n_estimators': 40, 'learning_rate': 1.8114473285278128}\n",
      "0.849 (+/-0.099) for {'n_estimators': 70, 'learning_rate': 1.8114473285278128}\n",
      "0.837 (+/-0.117) for {'n_estimators': 100, 'learning_rate': 1.8114473285278128}\n",
      "0.843 (+/-0.063) for {'n_estimators': 130, 'learning_rate': 1.8114473285278128}\n",
      "0.814 (+/-0.079) for {'n_estimators': 160, 'learning_rate': 1.8114473285278128}\n",
      "0.854 (+/-0.086) for {'n_estimators': 190, 'learning_rate': 1.8114473285278128}\n",
      "0.858 (+/-0.088) for {'n_estimators': 10, 'learning_rate': 4.0}\n",
      "0.843 (+/-0.108) for {'n_estimators': 40, 'learning_rate': 4.0}\n",
      "0.844 (+/-0.077) for {'n_estimators': 70, 'learning_rate': 4.0}\n",
      "0.831 (+/-0.089) for {'n_estimators': 100, 'learning_rate': 4.0}\n",
      "0.838 (+/-0.080) for {'n_estimators': 130, 'learning_rate': 4.0}\n",
      "0.839 (+/-0.096) for {'n_estimators': 160, 'learning_rate': 4.0}\n",
      "0.836 (+/-0.082) for {'n_estimators': 190, 'learning_rate': 4.0}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.82      0.84        40\n",
      "          1       0.94      0.95      0.95       119\n",
      "\n",
      "avg / total       0.92      0.92      0.92       159\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_estimators': 10, 'learning_rate': 1.8114473285278128}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.827 (+/-0.082) for {'n_estimators': 10, 'learning_rate': 0.015625}\n",
      "0.819 (+/-0.097) for {'n_estimators': 40, 'learning_rate': 0.015625}\n",
      "0.822 (+/-0.076) for {'n_estimators': 70, 'learning_rate': 0.015625}\n",
      "0.834 (+/-0.082) for {'n_estimators': 100, 'learning_rate': 0.015625}\n",
      "0.823 (+/-0.102) for {'n_estimators': 130, 'learning_rate': 0.015625}\n",
      "0.826 (+/-0.101) for {'n_estimators': 160, 'learning_rate': 0.015625}\n",
      "0.830 (+/-0.111) for {'n_estimators': 190, 'learning_rate': 0.015625}\n",
      "0.829 (+/-0.095) for {'n_estimators': 10, 'learning_rate': 0.034502797302306626}\n",
      "0.833 (+/-0.094) for {'n_estimators': 40, 'learning_rate': 0.034502797302306626}\n",
      "0.819 (+/-0.057) for {'n_estimators': 70, 'learning_rate': 0.034502797302306626}\n",
      "0.827 (+/-0.064) for {'n_estimators': 100, 'learning_rate': 0.034502797302306626}\n",
      "0.834 (+/-0.096) for {'n_estimators': 130, 'learning_rate': 0.034502797302306626}\n",
      "0.829 (+/-0.059) for {'n_estimators': 160, 'learning_rate': 0.034502797302306626}\n",
      "0.836 (+/-0.081) for {'n_estimators': 190, 'learning_rate': 0.034502797302306626}\n",
      "0.827 (+/-0.081) for {'n_estimators': 10, 'learning_rate': 0.076188353387779711}\n",
      "0.811 (+/-0.098) for {'n_estimators': 40, 'learning_rate': 0.076188353387779711}\n",
      "0.830 (+/-0.098) for {'n_estimators': 70, 'learning_rate': 0.076188353387779711}\n",
      "0.833 (+/-0.098) for {'n_estimators': 100, 'learning_rate': 0.076188353387779711}\n",
      "0.819 (+/-0.074) for {'n_estimators': 130, 'learning_rate': 0.076188353387779711}\n",
      "0.820 (+/-0.080) for {'n_estimators': 160, 'learning_rate': 0.076188353387779711}\n",
      "0.830 (+/-0.080) for {'n_estimators': 190, 'learning_rate': 0.076188353387779711}\n",
      "0.822 (+/-0.103) for {'n_estimators': 10, 'learning_rate': 0.16823752407904449}\n",
      "0.827 (+/-0.084) for {'n_estimators': 40, 'learning_rate': 0.16823752407904449}\n",
      "0.822 (+/-0.074) for {'n_estimators': 70, 'learning_rate': 0.16823752407904449}\n",
      "0.832 (+/-0.082) for {'n_estimators': 100, 'learning_rate': 0.16823752407904449}\n",
      "0.836 (+/-0.062) for {'n_estimators': 130, 'learning_rate': 0.16823752407904449}\n",
      "0.822 (+/-0.089) for {'n_estimators': 160, 'learning_rate': 0.16823752407904449}\n",
      "0.837 (+/-0.082) for {'n_estimators': 190, 'learning_rate': 0.16823752407904449}\n",
      "0.830 (+/-0.080) for {'n_estimators': 10, 'learning_rate': 0.37149857228423705}\n",
      "0.840 (+/-0.080) for {'n_estimators': 40, 'learning_rate': 0.37149857228423705}\n",
      "0.841 (+/-0.121) for {'n_estimators': 70, 'learning_rate': 0.37149857228423705}\n",
      "0.818 (+/-0.065) for {'n_estimators': 100, 'learning_rate': 0.37149857228423705}\n",
      "0.836 (+/-0.106) for {'n_estimators': 130, 'learning_rate': 0.37149857228423705}\n",
      "0.837 (+/-0.070) for {'n_estimators': 160, 'learning_rate': 0.37149857228423705}\n",
      "0.813 (+/-0.080) for {'n_estimators': 190, 'learning_rate': 0.37149857228423705}\n",
      "0.809 (+/-0.087) for {'n_estimators': 10, 'learning_rate': 0.82033535600763752}\n",
      "0.830 (+/-0.096) for {'n_estimators': 40, 'learning_rate': 0.82033535600763752}\n",
      "0.830 (+/-0.076) for {'n_estimators': 70, 'learning_rate': 0.82033535600763752}\n",
      "0.841 (+/-0.091) for {'n_estimators': 100, 'learning_rate': 0.82033535600763752}\n",
      "0.826 (+/-0.089) for {'n_estimators': 130, 'learning_rate': 0.82033535600763752}\n",
      "0.841 (+/-0.095) for {'n_estimators': 160, 'learning_rate': 0.82033535600763752}\n",
      "0.823 (+/-0.100) for {'n_estimators': 190, 'learning_rate': 0.82033535600763752}\n",
      "0.844 (+/-0.049) for {'n_estimators': 10, 'learning_rate': 1.8114473285278128}\n",
      "0.841 (+/-0.051) for {'n_estimators': 40, 'learning_rate': 1.8114473285278128}\n",
      "0.827 (+/-0.091) for {'n_estimators': 70, 'learning_rate': 1.8114473285278128}\n",
      "0.841 (+/-0.088) for {'n_estimators': 100, 'learning_rate': 1.8114473285278128}\n",
      "0.834 (+/-0.079) for {'n_estimators': 130, 'learning_rate': 1.8114473285278128}\n",
      "0.819 (+/-0.090) for {'n_estimators': 160, 'learning_rate': 1.8114473285278128}\n",
      "0.820 (+/-0.100) for {'n_estimators': 190, 'learning_rate': 1.8114473285278128}\n",
      "0.825 (+/-0.103) for {'n_estimators': 10, 'learning_rate': 4.0}\n",
      "0.827 (+/-0.065) for {'n_estimators': 40, 'learning_rate': 4.0}\n",
      "0.834 (+/-0.091) for {'n_estimators': 70, 'learning_rate': 4.0}\n",
      "0.825 (+/-0.097) for {'n_estimators': 100, 'learning_rate': 4.0}\n",
      "0.827 (+/-0.098) for {'n_estimators': 130, 'learning_rate': 4.0}\n",
      "0.825 (+/-0.097) for {'n_estimators': 160, 'learning_rate': 4.0}\n",
      "0.806 (+/-0.090) for {'n_estimators': 190, 'learning_rate': 4.0}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.88      0.86        40\n",
      "          1       0.96      0.95      0.95       119\n",
      "\n",
      "avg / total       0.93      0.93      0.93       159\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    ## Used to fit the Stump estimator parameters\n",
    "    #base_estimator=RandomForestClassifier(max_depth=None, min_samples_split=2, \n",
    "    #                                      min_samples_leaf=1, n_jobs=4, class_weight='balanced')\n",
    "    \n",
    "    #base_estimator=RandomForestClassifier(max_depth=None, max_features=22, n_estimators=50, n_jobs=4, \n",
    "    #                                          min_samples_split=1, class_weight=None)\n",
    "    \n",
    "    #base_estimator=ExtraTreesClassifier(max_depth=None, min_samples_split=1, n_jobs=4, max_features=8, n_estimators=10)\n",
    "    \n",
    "    #base_estimator=RandomForestClassifier(max_depth=None, min_samples_split=1, n_jobs=4)\n",
    "    #base_estimator=ExtraTreesClassifier(max_depth=None, min_samples_split=1, n_jobs=4)\n",
    "    \n",
    "    ## Used to fit AdaBoost\n",
    "    #base_estimator=RandomForestClassifier(max_depth=None, min_samples_split=2, min_samples_leaf=1, n_jobs=4, \n",
    "    #                       class_weight='balanced', max_features=14, n_estimators=10)\n",
    "    \n",
    "    ada_real = AdaBoostClassifier(base_estimator=base_estimator, algorithm=\"SAMME.R\")\n",
    "\n",
    "    #clf = GridSearchCV(base_estimator, tuned_parameters, cv=5, scoring='%s_macro' % score)\n",
    "    clf = GridSearchCV(ada_real, tuned_parameters, cv=5, scoring='%s_macro' % score)\n",
    "    clf.fit(X_normalized, y_train)\n",
    "    #clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    #y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    y_true, y_pred = y_cv, clf.predict(X_cv_normalized)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=22, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=1, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=50, n_jobs=4, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "          learning_rate=0.0675, n_estimators=500, random_state=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ada_tmp = AdaBoostClassifier(base_estimator=RandomForestClassifier(max_depth=None, min_samples_split=3, min_samples_leaf=2, \n",
    "#                                n_jobs=4, class_weight='balanced', max_features=8, n_estimators=10), \n",
    "#                             algorithm=\"SAMME.R\", n_estimators=500, learning_rate=0.0675)\n",
    "\n",
    "ada_tmp = AdaBoostClassifier(base_estimator=RandomForestClassifier(max_depth=None, min_samples_split=1, n_jobs=4, \n",
    "                              max_features=22, n_estimators=50), algorithm=\"SAMME.R\", \n",
    "                              n_estimators=500, learning_rate=0.0675)\n",
    "\n",
    "# v_10\n",
    "#ada_tmp = AdaBoostClassifier(base_estimator=ExtraTreesClassifier(max_depth=None, min_samples_split=1, n_jobs=4,\n",
    "#                                                                  max_features=8, n_estimators=10),\n",
    "#                            algorithm=\"SAMME.R\", n_estimators=10, learning_rate=1.8114473285278128)\n",
    "\n",
    "ada_tmp.fit(X_normalized,y_train)\n",
    "#clf.param_grid = {'n_estimators': 200, 'learning_rate': 0.1}\n",
    "#print(clf.score(X_test_normalized,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96250000000000002"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_tmp.score(X_test_normalized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97499999999999998"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v_10\n",
    "ada_tmp.score(X_test_normalized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04380797,  0.05622355,  0.03498273,  0.04082949,  0.12295559,\n",
       "        0.10337428,  0.02993285,  0.03057933,  0.02438626,  0.02455615,\n",
       "        0.03117653,  0.03084531,  0.09665527,  0.07359508,  0.06643316,\n",
       "        0.00848542,  0.01526771,  0.00669005,  0.03094512,  0.047321  ,\n",
       "        0.0241514 ,  0.02955126,  0.0041441 ,  0.0231104 ])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_tmp.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features=14,\n",
      "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=4,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "ex = RandomForestClassifier(max_depth=None, min_samples_split=2, min_samples_leaf=1, n_jobs=4, \n",
    "                            class_weight='balanced', max_features=14, n_estimators=10)\n",
    "\n",
    "print(ex.fit(X_normalized, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7625\n"
     ]
    }
   ],
   "source": [
    "print(ex.score(X_test_normalized,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X, y = X_train, X_target\n",
    "indices = np.arange(y.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X, y = X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train scores \n",
      "\n",
      " [[ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]] \n",
      "\n",
      "Valid scores \n",
      "\n",
      " [[ 0.73584906  0.74213836  0.71698113  0.6918239   0.76582278]\n",
      " [ 0.72955975  0.72955975  0.72955975  0.71069182  0.75316456]\n",
      " [ 0.75471698  0.73584906  0.73584906  0.71698113  0.74683544]\n",
      " [ 0.7672956   0.73584906  0.72955975  0.71698113  0.75949367]\n",
      " [ 0.74842767  0.71698113  0.73584906  0.71069182  0.75949367]\n",
      " [ 0.74213836  0.73584906  0.73584906  0.70440252  0.76582278]]\n"
     ]
    }
   ],
   "source": [
    "param_range = [1,2,4,8,12,13]\n",
    "train_scores, test_scores = validation_curve(RandomForestClassifier(n_estimators=1000), X_train, X_target, param_name=\"max_features\", param_range=param_range,\n",
    "                                             cv=5, scoring=\"accuracy\", n_jobs=1)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "print(\"Train scores \\n\\n %s \\n\" % train_scores)\n",
    "print(\"Valid scores \\n\\n %s\" % test_scores)\n",
    "\n",
    "plt.title(\"Validation Curve with SVM\")\n",
    "plt.xlabel(\"$\\gamma$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "title = \"Learning Curves Random Forest Classifier 400 / 13\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = RandomForestClassifier(n_estimators=400, max_features=13, max_depth=None, min_samples_split=1)\n",
    "plot_learning_curve(estimator, title, X_train, X_target, ylim=(0.5, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "title = \"Learning Curves AdaBoost\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = AdaBoostClassifier(\n",
    "    base_estimator=RandomForestClassifier(max_depth=None, min_samples_split=1, n_estimators=100, max_features=12),\n",
    "    learning_rate=0.8,\n",
    "    n_estimators=600,\n",
    "    algorithm=\"SAMME.R\")\n",
    "plot_learning_curve(estimator, title, X_train, X_target, (0.5, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('C:\\Pedro\\workspace\\Acc-challenge\\\\test_data_v3_3.csv')\n",
    "#test = pd.read_csv('C:\\Pedro\\workspace\\Acc-challenge\\\\test_data_v3_2.csv')\n",
    "#test = test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "test['PlayerName'].apply(lambda x: pname_mapping.update(zip([x],[len(pname_mapping.keys())])) \n",
    "                         if pname_mapping.has_key(x) == False else None)\n",
    "\n",
    "test['StartingPositionDesc'].apply(lambda x: spos_mapping.update(zip([x],[len(spos_mapping.keys())])) \n",
    "                         if spos_mapping.has_key(x) == False else None)\n",
    "\n",
    "test['EndPositionDesc'].apply(lambda x: epos_mapping.update(zip([x],[len(epos_mapping.keys())])) \n",
    "                         if epos_mapping.has_key(x) == False else None)\n",
    "\n",
    "test['ActionTypeDesc'].apply(lambda x: atype_mapping.update(zip([x],[len(atype_mapping.keys())])) \n",
    "                         if atype_mapping.has_key(x) == False else None)\n",
    "\n",
    "test['Summary'].apply(lambda x: summary_mapping.update(zip([x],[len(summary_mapping.keys())])) \n",
    "                         if summary_mapping.has_key(x) == False else None)\n",
    "\n",
    "test['StadiumName'].apply(lambda x: stad_mapping.update(zip([x],[len(stad_mapping.keys())])) \n",
    "                         if stad_mapping.has_key(x) == False else None)\n",
    "\n",
    "test['Team'].apply(lambda x: team_mapping.update(zip([x],[len(team_mapping.keys())])) \n",
    "                         if team_mapping.has_key(x) == False else None)\n",
    "\n",
    "test['ZoneDescription'].apply(lambda x: zone_mapping.update(zip([x],[len(zone_mapping.keys())])) \n",
    "                         if zone_mapping.has_key(x) == False else None)\n",
    "\n",
    "test['StadiumName_Val'] = test['StadiumName'].map(stad_mapping).astype(int)\n",
    "test['Team_Val'] = test['Team'].map(team_mapping).astype(int)\n",
    "test['HomeTeam_Val'] = test['HomeTeam'].map(team_mapping).astype(int)\n",
    "test['AwayTeam_Val'] = test['AwayTeam'].map(team_mapping).astype(int)\n",
    "test['ZoneDescription_Val'] = test['ZoneDescription'].map(zone_mapping).astype(int)\n",
    "\n",
    "test['PlayerName_Val'] = test['PlayerName'].map(pname_mapping).astype(int)\n",
    "test['StartingPositionDesc_Val'] = test['StartingPositionDesc'].map(spos_mapping).astype(int)\n",
    "test['ActionTypeDesc_Val'] = test['ActionTypeDesc'].map(atype_mapping).astype(int)\n",
    "test['EndPositionDesc_Val'] = test['EndPositionDesc'].map(epos_mapping).astype(int)\n",
    "test['Summary_Val'] = test['Summary'].map(summary_mapping).astype(int)\n",
    "\n",
    "\n",
    "Test_ids = test['RecordID']\n",
    "Test_predict = test.drop(['RecordID','PlayerName','StartingPositionDesc',\n",
    "                              'EndPositionDesc','Summary','ActionTypeDesc','StadiumName',\n",
    "                                'Team','HomeTeam','AwayTeam','ZoneDescription','Visibility','Pressure',\n",
    "                                 'Humidity','DewPoint','CloudCover','Temperature'], axis=1).values\n",
    "\n",
    "#Test_predict = test.drop(['PlayerName','StartingPositionDesc','EndPositionDesc','Summary',\n",
    "#                             'Summary_Val','EndPositionDesc_Val','End_Y','ActionTypeDesc'], axis=1)\n",
    "\n",
    "# No need to shuffle prediction set (we are not fitting it)\n",
    "#np.random.seed(0)\n",
    "#Test_sorted = Test_predict.reindex(np.random.permutation(Test_predict.index))\n",
    "#X_test = Test_sorted.drop(['RecordID'], axis=1).values\n",
    "\n",
    "# Apply same transformation to test data (normalization)\n",
    "test_set_normalized = scaler.transform(Test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=12, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=1, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "          learning_rate=0.8, n_estimators=400, random_state=None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_real = AdaBoostClassifier(\n",
    "    base_estimator=RandomForestClassifier(max_depth=None, min_samples_split=1, n_estimators=100, max_features=12),\n",
    "    learning_rate=0.8,\n",
    "    n_estimators=400,\n",
    "    algorithm=\"SAMME.R\")\n",
    "    \n",
    "ada_real.fit(X_normalized, X_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0\n",
      " 1 1 1 1 1]\n",
      "\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      " Goal Kick Missed       0.93      0.68      0.79        19\n",
      "Goal Kick Success       0.91      0.98      0.94        60\n",
      "\n",
      "      avg / total       0.91      0.91      0.91        79\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pred_y = ada_real.predict(X_cv_normalized)\n",
    "pred_y = ada_tmp.predict(X_cv_normalized)\n",
    "print(pred_y)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "target_names = ['Goal Kick Missed', 'Goal Kick Success']\n",
    "print(classification_report(y_cv, pred_y, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#pred_y_test = ada_real.predict(test_set_normalized)\n",
    "pred_y_test = ada_tmp.predict(test_set_normalized) # Already fitted earlier\n",
    "\n",
    "print(pred_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reverse_map = {0: 'Goal Kick Missed', 1: 'Goal Kick Success'}\n",
    "\n",
    "result = pd.DataFrame({'RowId': Test_ids})\n",
    "result['Prediction'] = pred_y_test.T\n",
    "result['Prediction'] = result['Prediction'].map(reverse_map)\n",
    "result\n",
    "\n",
    "result.to_excel('PedroCastanha_AdaBoost_Normalized_NewMap_RandomTree_MoreData_NewParams_Score_v11.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 1 (0.127705)\n",
      "2. feature 0 (0.121489)\n",
      "3. feature 13 (0.079950)\n",
      "4. feature 11 (0.078692)\n",
      "5. feature 7 (0.077209)\n",
      "6. feature 9 (0.074318)\n",
      "7. feature 5 (0.069823)\n",
      "8. feature 8 (0.065933)\n",
      "9. feature 6 (0.064566)\n",
      "10. feature 2 (0.063508)\n",
      "11. feature 3 (0.050709)\n",
      "12. feature 4 (0.050206)\n",
      "13. feature 10 (0.048022)\n",
      "14. feature 12 (0.027871)\n"
     ]
    }
   ],
   "source": [
    "forest = ExtraTreesClassifier(n_estimators=250, random_state=0)\n",
    "forest.fit(X_train, X_target)\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), indices)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
