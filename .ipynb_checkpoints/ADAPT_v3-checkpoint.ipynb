{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:\\Pedro\\workspace\\Acc-challenge\\data_v2.csv')\n",
    "#train = train.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "pname_mapping = dict(zip(sorted(train['PlayerName'].unique()), range(0, len(sorted(train['PlayerName'].unique())) + 1)))\n",
    "aname_mapping = dict(zip(sorted(train['ActionName'].unique()), range(0, len(sorted(train['ActionName'].unique())) + 1)))\n",
    "spos_mapping = dict(zip(sorted(train['StartingPositionDesc'].unique()), range(0, len(sorted(train['StartingPositionDesc'].unique())) + 1)))\n",
    "epos_mapping = dict(zip(sorted(train['EndPositionDesc'].unique()), range(0, len(sorted(train['EndPositionDesc'].unique())) + 1)))\n",
    "summary_mapping = dict(zip(sorted(train['Summary'].unique()), range(0, len(sorted(train['Summary'].unique())) + 1)))\n",
    "\n",
    "train['PlayerName_Val'] = train['PlayerName'].map(pname_mapping).astype(int)\n",
    "train['ActionName_Val'] = train['ActionName'].map(aname_mapping).astype(int)\n",
    "train['StartingPositionDesc_Val'] = train['StartingPositionDesc'].map(spos_mapping).astype(int)\n",
    "train['EndPositionDesc_Val'] = train['EndPositionDesc'].map(epos_mapping).astype(int)\n",
    "train['Summary_Val'] = train['Summary'].map(summary_mapping).astype(int)\n",
    "\n",
    "np.random.seed(0)\n",
    "train = train.reindex(np.random.permutation(train.index)) #Shuffling\n",
    "\n",
    "X_target = train['ActionName_Val'].values\n",
    "#X_train = train.drop(['ActionName_Val','ActionName','PlayerName','StartingPositionDesc',\n",
    "#                      'EndPositionDesc','Summary','RecordID'], axis=1).values\n",
    "\n",
    "\n",
    "X_train = train.drop(['ActionName_Val','ActionName','PlayerName','StartingPositionDesc',\n",
    "                      'EndPositionDesc','Summary','RecordID','Summary_Val','EndPositionDesc_Val','End_Y'], axis=1)\n",
    "\n",
    "#train_features = train_test.values[:, 1:48]\n",
    "#train_features = train_test.iloc[:,[10,11,17,12,35,34,6,8,5,9]].values\n",
    "#train_features = train_test.iloc[:,[10,11,15,34,6]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X_train, X_target, test_size=0.5, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, X_target, test_size=0.3, random_state=0)\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(X_train)\n",
    "#X_normalized = scaler.transform(X_train)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'max_features': [2,4,8,10,12,13], 'n_estimators': [1, 10, 100, 1000]}]\n",
    "scores = ['precision', 'recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'max_features': 8, 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.585 (+/-0.032) for {'max_features': 2, 'n_estimators': 1}\n",
      "0.569 (+/-0.101) for {'max_features': 2, 'n_estimators': 10}\n",
      "0.633 (+/-0.048) for {'max_features': 2, 'n_estimators': 100}\n",
      "0.626 (+/-0.068) for {'max_features': 2, 'n_estimators': 1000}\n",
      "0.561 (+/-0.042) for {'max_features': 4, 'n_estimators': 1}\n",
      "0.593 (+/-0.051) for {'max_features': 4, 'n_estimators': 10}\n",
      "0.610 (+/-0.086) for {'max_features': 4, 'n_estimators': 100}\n",
      "0.623 (+/-0.078) for {'max_features': 4, 'n_estimators': 1000}\n",
      "0.559 (+/-0.122) for {'max_features': 8, 'n_estimators': 1}\n",
      "0.649 (+/-0.113) for {'max_features': 8, 'n_estimators': 10}\n",
      "0.605 (+/-0.112) for {'max_features': 8, 'n_estimators': 100}\n",
      "0.610 (+/-0.081) for {'max_features': 8, 'n_estimators': 1000}\n",
      "0.576 (+/-0.077) for {'max_features': 10, 'n_estimators': 1}\n",
      "0.589 (+/-0.122) for {'max_features': 10, 'n_estimators': 10}\n",
      "0.633 (+/-0.126) for {'max_features': 10, 'n_estimators': 100}\n",
      "0.614 (+/-0.129) for {'max_features': 10, 'n_estimators': 1000}\n",
      "0.526 (+/-0.139) for {'max_features': 12, 'n_estimators': 1}\n",
      "0.602 (+/-0.066) for {'max_features': 12, 'n_estimators': 10}\n",
      "0.613 (+/-0.070) for {'max_features': 12, 'n_estimators': 100}\n",
      "0.632 (+/-0.123) for {'max_features': 12, 'n_estimators': 1000}\n",
      "0.568 (+/-0.123) for {'max_features': 13, 'n_estimators': 1}\n",
      "0.587 (+/-0.084) for {'max_features': 13, 'n_estimators': 10}\n",
      "0.611 (+/-0.103) for {'max_features': 13, 'n_estimators': 100}\n",
      "0.638 (+/-0.141) for {'max_features': 13, 'n_estimators': 1000}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.37      0.40        62\n",
      "          1       0.79      0.84      0.81       177\n",
      "\n",
      "avg / total       0.70      0.72      0.71       239\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'max_features': 10, 'n_estimators': 10}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.531 (+/-0.102) for {'max_features': 2, 'n_estimators': 1}\n",
      "0.569 (+/-0.040) for {'max_features': 2, 'n_estimators': 10}\n",
      "0.557 (+/-0.070) for {'max_features': 2, 'n_estimators': 100}\n",
      "0.554 (+/-0.049) for {'max_features': 2, 'n_estimators': 1000}\n",
      "0.561 (+/-0.065) for {'max_features': 4, 'n_estimators': 1}\n",
      "0.553 (+/-0.121) for {'max_features': 4, 'n_estimators': 10}\n",
      "0.570 (+/-0.097) for {'max_features': 4, 'n_estimators': 100}\n",
      "0.575 (+/-0.062) for {'max_features': 4, 'n_estimators': 1000}\n",
      "0.567 (+/-0.054) for {'max_features': 8, 'n_estimators': 1}\n",
      "0.573 (+/-0.122) for {'max_features': 8, 'n_estimators': 10}\n",
      "0.587 (+/-0.085) for {'max_features': 8, 'n_estimators': 100}\n",
      "0.574 (+/-0.085) for {'max_features': 8, 'n_estimators': 1000}\n",
      "0.523 (+/-0.054) for {'max_features': 10, 'n_estimators': 1}\n",
      "0.613 (+/-0.082) for {'max_features': 10, 'n_estimators': 10}\n",
      "0.573 (+/-0.063) for {'max_features': 10, 'n_estimators': 100}\n",
      "0.569 (+/-0.083) for {'max_features': 10, 'n_estimators': 1000}\n",
      "0.548 (+/-0.100) for {'max_features': 12, 'n_estimators': 1}\n",
      "0.572 (+/-0.045) for {'max_features': 12, 'n_estimators': 10}\n",
      "0.585 (+/-0.085) for {'max_features': 12, 'n_estimators': 100}\n",
      "0.583 (+/-0.085) for {'max_features': 12, 'n_estimators': 1000}\n",
      "0.555 (+/-0.089) for {'max_features': 13, 'n_estimators': 1}\n",
      "0.582 (+/-0.091) for {'max_features': 13, 'n_estimators': 10}\n",
      "0.562 (+/-0.073) for {'max_features': 13, 'n_estimators': 100}\n",
      "0.587 (+/-0.097) for {'max_features': 13, 'n_estimators': 1000}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.40      0.42        62\n",
      "          1       0.80      0.82      0.81       177\n",
      "\n",
      "avg / total       0.71      0.72      0.71       239\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=5, scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "    #clf.fit(X_normalized, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X, y = X_train, X_target\n",
    "indices = np.arange(y.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X, y = X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train scores \n",
      "\n",
      " [[ 0.98740157  0.98740157  0.99212598  0.98582677  0.99056604]\n",
      " [ 0.98740157  0.98267717  0.99212598  0.99212598  0.99371069]\n",
      " [ 0.98267717  0.98897638  0.99527559  0.98425197  0.99213836]\n",
      " [ 0.98897638  0.99055118  0.98267717  0.98267717  0.98584906]\n",
      " [ 0.98110236  0.99212598  0.99370079  0.98267717  0.99056604]\n",
      " [ 0.98110236  0.98267717  0.98582677  0.99212598  0.98584906]] \n",
      "\n",
      "Valid scores \n",
      "\n",
      " [[ 0.77987421  0.6918239   0.69811321  0.64779874  0.70253165]\n",
      " [ 0.70440252  0.71069182  0.59119497  0.72955975  0.74050633]\n",
      " [ 0.77358491  0.72955975  0.69811321  0.69811321  0.71518987]\n",
      " [ 0.70440252  0.71698113  0.71069182  0.66037736  0.75316456]\n",
      " [ 0.79245283  0.71069182  0.67295597  0.68553459  0.75316456]\n",
      " [ 0.74842767  0.69811321  0.67295597  0.71698113  0.73417722]]\n"
     ]
    }
   ],
   "source": [
    "#param_range = np.logspace(-6, -1, 15)\n",
    "#param_range = [1,2,4,8,12,16]\n",
    "param_range = [1,2,4,8,12,13]\n",
    "train_scores, test_scores = validation_curve(RandomForestClassifier(n_estimators=10), X_train, X_target, param_name=\"max_features\", param_range=param_range,\n",
    "                                             cv=5, scoring=\"accuracy\", n_jobs=1)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "print(\"Train scores \\n\\n %s \\n\" % train_scores)\n",
    "print(\"Valid scores \\n\\n %s\" % test_scores)\n",
    "\n",
    "plt.title(\"Validation Curve with SVM\")\n",
    "plt.xlabel(\"$\\gamma$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "title = \"Learning Curves (Naive Bayes)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = RandomForestClassifier(n_estimators=10, max_features=10)\n",
    "plot_learning_curve(estimator, title, X_train, X_target, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "title = \"Learning Curves (RFC feat = 10, est=10)\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = RandomForestClassifier(n_estimators=10, max_features=10)\n",
    "plot_learning_curve(estimator, title, X_train, X_target, (0.7, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('C:\\Pedro\\workspace\\Acc-challenge\\\\test_data_v2.csv')\n",
    "#test = test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "t_pname_mapping = dict(zip(sorted(test['PlayerName'].unique()), range(0, len(sorted(test['PlayerName'].unique())) + 1)))\n",
    "t_spos_mapping = dict(zip(sorted(test['StartingPositionDesc'].unique()), range(0, len(sorted(test['StartingPositionDesc'].unique())) + 1)))\n",
    "t_epos_mapping = dict(zip(sorted(test['EndPositionDesc'].unique()), range(0, len(sorted(test['EndPositionDesc'].unique())) + 1)))\n",
    "t_summary_mapping = dict(zip(sorted(test['Summary'].unique()), range(0, len(sorted(test['Summary'].unique())) + 1)))\n",
    "\n",
    "test['PlayerName_Val'] = test['PlayerName'].map(t_pname_mapping).astype(int)\n",
    "test['StartingPositionDesc_Val'] = test['StartingPositionDesc'].map(t_spos_mapping).astype(int)\n",
    "test['EndPositionDesc_Val'] = test['EndPositionDesc'].map(t_epos_mapping).astype(int)\n",
    "test['Summary_Val'] = test['Summary'].map(t_summary_mapping).astype(int)\n",
    "\n",
    "#Test_predict = test.drop(['PlayerName','StartingPositionDesc','EndPositionDesc','Summary'], axis=1)\n",
    "Test_predict = test.drop(['PlayerName','StartingPositionDesc','EndPositionDesc','Summary',\n",
    "                             'Summary_Val','EndPositionDesc_Val','End_Y'], axis=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "Test_sorted = Test_predict.reindex(np.random.permutation(Test_predict.index))\n",
    "X_test = Test_sorted.drop(['RecordID'], axis=1).values\n",
    "\n",
    "#test_features = test_set.iloc[:,[10,11,17,12,35,34,6,8,5,9]].values\n",
    "#test_features = test_set.iloc[:,[10,11,15,34,6]].values\n",
    "#test_features = test_set.values[:,1:48]\n",
    "#X_test = scaler.transform(test_features)  # apply same transformation to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=10, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_features=10, n_estimators=10)\n",
    "clf.fit(X_train, X_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = clf.predict(X_test)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'RecordID': Test_sorted['RecordID']})\n",
    "result['ActionName'] = pred_y.T\n",
    "result\n",
    "result.to_excel('PedroCastanha_v5.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 1 (0.126497)\n",
      "2. feature 0 (0.121862)\n",
      "3. feature 13 (0.081340)\n",
      "4. feature 12 (0.073890)\n",
      "5. feature 8 (0.070335)\n",
      "6. feature 10 (0.068315)\n",
      "7. feature 6 (0.065590)\n",
      "8. feature 9 (0.060387)\n",
      "9. feature 7 (0.057086)\n",
      "10. feature 4 (0.045432)\n",
      "11. feature 11 (0.043139)\n",
      "12. feature 5 (0.042171)\n",
      "13. feature 2 (0.040435)\n",
      "14. feature 15 (0.038373)\n",
      "15. feature 3 (0.037797)\n",
      "16. feature 14 (0.027351)\n"
     ]
    }
   ],
   "source": [
    "forest = ExtraTreesClassifier(n_estimators=250, random_state=0)\n",
    "forest.fit(X, y)\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), indices)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
